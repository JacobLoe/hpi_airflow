# docker rmi jacobloe/airflow:0.1
# docker build -f Dockerfile_airflow -t jacobloe/airflow:0.1 .
# docker volume create --driver local --opt type=none --opt device=/home/jacob/Downloads/hpi/data --opt o=bind airflow_cache
# docker run --rm -it -v airflow_cache:/data -v /var/run/docker.sock:/var/run/docker.sock -p 8080:8080 --network ndd_subnet --name airflow jacobloe/airflow:0.1
# --entrypoint /bin/bash
# airflow scheduler & airflow webserver -p 8080
# apt update && apt install -y vim && vim airflow.cfg

# docker run --rm -it -v airflow_cache:/data --entrypoint /bin/bash jacobloe/shot_detection:0.7
FROM python:3.8-slim-buster

ENV DEBIAN_FRONTEND noninteractive

RUN apt-get update && apt-get install -y build-essential

# install airflow for python 3.8
RUN pip3 install --trusted-host pypi.python.org apache-airflow==1.10.12 --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-1.10.12/constraints-3.8.txt"
# is needed for docker_operator.py
RUN pip3 install docker==4.3.1

# initialise the database for airflow
# this also creates the airflow folder with the configuration in root
RUN airflow initdb

# edit the airflow configuration file to let airflow exclude example dags
RUN sed -i '/load_examples = True/c load_examples = False' /root/airflow/airflow.cfg
# now all dags can be triggered without needing to unpause them first
RUN sed -i '/dags_are_paused_at_creation = True/c dags_are_paused_at_creation = False' /root/airflow/airflow.cfg
# FIXME this done temporarily to allow all connections to the server, ideally this would be changed to work with username/password
RUN sed -i '/auth_backend = airflow.api.auth.backend.deny_all/c auth_backend = airflow.api.auth.backend.default' /root/airflow/airflow.cfg
# save the airflow logs in the docker volume, the 's# # #' is needed for the command to affect the path
RUN sed -i 's#base_log_folder = /root/airflow/logs#base_log_folder = /data/airflow/logs#' /root/airflow/airflow.cfg

# create the folder in which airflow is lokking for dags
RUN mkdir /root/airflow/dags

# dag scripts are copied into the default folder for airflow dag scripts
COPY airflow_shotdetection.py /root/airflow/dags
COPY airflow_feature_extraction.py /root/airflow/dags
COPY airflow_aspect_ratio_extraction.py /root/airflow/dags
COPY airflow_optical_flow.py /root/airflow/dags

# copy the scripts that are used by all dags for tasks
COPY docker_operator.py /root/airflow/dags
COPY get_video.py /root/airflow/dags

# initialising the database again now excludes the example dags and adds the new dags as specified in the airflow.cfg
RUN airflow initdb

# expose port for the airflow webserver
EXPOSE 8080

VOLUME "/data"

WORKDIR /root

# start the scheduler so dags can be triggered and start the webserver to allow supervision
ENTRYPOINT airflow scheduler & airflow webserver -p 8080